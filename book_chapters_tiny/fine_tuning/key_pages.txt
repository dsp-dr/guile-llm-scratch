=== PAGE 0 ===
199 6.7 Fine-tuning the model on supervised data
As we can see based on the sharp downward slope in figure 6.16, the model is learning
well from the training data, an d there is little to no indication of overfitting; that is,
there is no noticeable gap between th e training and validation set losses.
Using the same plot_values  function, let’s now plot th e classification accuracies:
epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))
examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))
plot_values(
    epochs_tensor, examples_seen_tensor, train_accs, val_accs,
    label="accuracy"
)
Figure 6.17 graphs the resulting accuracy. The model achieves a relatively high training
and validation accuracy after epochs 4 an d 5. Importantly, we previously set eval_iter=5Choosing the number of epochs 
Earlier, when we initiated the training, we set the number of epochs to five. The num-
ber of epochs depends on the dataset and the task’s difficulty, and there is no uni-
versal solution or recommendation, although an epoch number of five is usually agood starting point. If the model overfits af ter the first few epochs as a loss plot (see
figure 6.16), you may need to reduce the number of epochs. Conversely, if the trend-
line suggests that the validation loss could improve with further training, you shouldincrease the number of epochs. In this concrete case, five epochs is a reasonable
number as there are no signs of early overfitting, and the validation loss is close to 0.
Figure 6.17 Both the training accuracy (solid line) and the validation 
accuracy (dashed line) increase substantially in the early epochs and then plateau, achieving almost perfect accuracy scores of 1.0. The 
close proximity of the two lines throughout the epochs suggests that 
the model does not overfit the training data very much.
Licensed to Jason Walsh <manning@jwalsh.net>

=== PAGE 1 ===
200 CHAPTER  6Fine-tuning for classification
when using the train_classifier_simple  function, which means our estimations of
training and validation performance are ba sed on only five batches for efficiency
during training. 
 Now we must calculate the performance metrics for the traini ng, validation, and
test sets across the entire dataset by runni ng the following code, this time without
defining the eval_iter  value:
train_accuracy = calc_accuracy_loader(train_loader, model, device)
val_accuracy = calc_accuracy_loader(val_loader, model, device)
test_accuracy = calc_accuracy_loader(test_loader, model, device)
print(f"Training accuracy: {train_accuracy*100:.2f}%")
print(f"Validation accuracy: {val_accuracy*100:.2f}%")
print(f"Test accuracy: {test_accuracy*100:.2f}%")
The resulting accuracy values are
Training accuracy: 97.21%Validation accuracy: 97.32%
Test accuracy: 95.67%
The training and test set performances are almost identical. Th e slight discrepancy
between the training and test set accuracies  suggests minimal overfitting of the train-
ing data. Typically, the validation set accura cy is somewhat higher than the test set
accuracy because the model development often involves tuning hyperparameters toperform well on the validation set, which might not generalize as effectively to the test
set. This situation is common, but the gap could potentially be minimized by adjusting
the model’s settings, such as increasing the dropout rate (
drop_rate ) or the weight_
decay  parameter in the optimizer configuration.
6.8 Using the LLM as a spam classifier
Having fine-tuned and evaluated the model,  we are now ready to classify spam mes-
sages (see figure 6.18). Let’s use our fine -tuned GPT-based spam classification model.
The following classify_review  function follows data preprocessing steps similar
to those we used in the SpamDataset  implemented earlier. Then, after processing
text into token IDs, the fu nction uses the model to pr edict an integer class label,
similar to what we implemented in sectio n 6.6, and then returns the corresponding
class name.
 
 
  
 
 
Licensed to Jason Walsh <manning@jwalsh.net>

=== PAGE 2 ===
201 6.8 Using the LLM as a spam classifier
def classify_review(
        text, model, tokenizer, device, max_length=None,
        pad_token_id=50256):    model.eval()
    input_ids = tokenizer.encode(text)         
    supported_context_length = model.pos_emb.weight.shape[0]
    input_ids = input_ids[:min(             
        max_length, supported_context_length    )]
    input_ids += [pad_token_id] * (max_length - len(input_ids))   
    
    input_tensor = torch.tensor(
        input_ids, device=device    ).unsqueeze(0)             
    
    with torch.no_grad():                                       logits = model(input_tensor)[:, -1, :]    
    predicted_label = torch.argmax(logits, dim=-1).item()
    return "spam" if predicted_label == 1 else "not spam"    Listing 6.12 Using the model to classify new textsStage 1:
Dataset preparation
1) Download
the dataset
2) Preprocess
dataset
3) Create data
loaders4) Initialize
model
5) Load pretrained
weights
6) Modify model
for ﬁne-tuning
7) Implement
evaluation utilities8) Fine-tune
model
9) Evaluate
ﬁne-tuned model
10) Use model
on new data10) Use model
on new dataStage 2:
Model setupStage 3:
Model ﬁne-tuning
and usage
We are ready to try the model
on new text messages.
Figure 6.18 The three-stage process for classification fine-tuning our LLM. Step 
10 is the final step of stage 3—using the fine-tuned model to classify new spam 
messages.
Prepares inputs 
to the model
Truncates sequences if 
they are too long
Pads sequences
to the longest
sequenceAdds batch 
dimension
Models inference 
without gradient tracking
Logits of the la st output token Returns the classified result
Licensed to Jason Walsh <manning@jwalsh.net>

=== PAGE 5 ===
204Fine-tuning to follow
instructions
Previously, we implemented the LLM arch itecture, carried out pretraining, and
imported pretrained weights from external sources into our model. Then, we
focused on fine-tuning our LLM for a spec ific classification task: distinguishing
between spam and non-spam text messages. Now we’ll implement the process for
fine-tuning an LLM to follow human inst ructions, as illustrated in figure 7.1.
Instruction fine-tuning is one of the main  techniques behind developing LLMs for
chatbot applications , personal assistants, and other conversational tasks.This chapter covers
The instruction fine-tuning process of LLMs
Preparing a dataset for supervised instruction 
fine-tuning
Organizing instruction data in training batches
Loading a pretrained LLM and fine-tuning it to follow human instructions
Extracting LLM-generated instruction responses for evaluation
Evaluating an instruction-fine-tuned LLM
Licensed to Jason Walsh <manning@jwalsh.net>

=== PAGE 10 ===
209 7.2 Preparing a dataset for supervised instruction fine-tuning
example formats, often referred to as prompt styles , used in the training of notable
LLMs such as Alpaca and Phi-3. 
 Alpaca was one of the early LLMs to public ly detail its instruction fine-tuning pro-
cess. Phi-3, developed by Microsoft, is incl uded to demonstrate the diversity in prompt
styles. The rest of this chapter uses the Alpa ca prompt style since it is one of the most
popular ones, largely because it helped defi ne the original approach to fine-tuning.
Let’s define a format_input  function that we can use to convert the entries in the
data  list into the Alpaca-style input format. 
def format_input(entry):
    instruction_text = (        f"Below is an instruction that describes a task. "
        f"Write a response that appropriately completes the request."
        f"\n\n### Instruction:\n{entry['instruction']}"    )
    
    input_text = (        f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""
    )
    return instruction_text + input_text
This format_input  function takes a dictionary entry  as input and constructs a format-
ted string. Let’s test it to dataset entry data[50] , which we looked at earlier:
model_input = format_input(data[50])
desired_response = f"\n\n### Response:\n{data[50]['output']}"
print(model_input + desired_response)
The formatted input looks like as follows:
Below is an instruction that describes a task. Write a response that appropriately completes the request.
### Instruction:
Identify the correct spelling of the following word.
### Input:
Ocassion
### Response:
The correct spelling is 'Occasion.'Exercise 7.1 Changing prompt styles 
After fine-tuning the model with the Alpaca prompt style, try the Phi-3 prompt style
shown in figure 7.4 and observe whether it affects the response quality of the model.
Listing 7.2 Implementing the prompt formatting function
Licensed to Jason Walsh <manning@jwalsh.net>

=== PAGE 15 ===
214 CHAPTER  7Fine-tuning to follow instructions
Similar to the approach used for classification  fine-tuning, we want to accelerate train-
ing by collecting multiple training examples  in a batch, which necessitates padding all
inputs to a similar length. As with cl assification fine-tuning, we use the <|endoftext|>
token as a padding token. 
 Instead of appending the <|endoftext|>  tokens to the text inputs, we can append
the token ID corresponding to <|endoftext|>  to the pretokenized inputs directly. We
can use the tokenizer’s .encode  method on an <|endoftext|>  token to remind us
which token ID we should use:
import tiktoken
tokenizer = tiktoken.get_encoding("gpt2")
print(tokenizer.encode("<|endoftext|>", allowed_special={"<|endoftext|>"}))
The resulting token ID is 50256 . 
 Moving on to step 2.3 of the process (s ee figure 7.6), we adopt a more sophisti-
cated approach by developing a custom coll ate function that we can pass to the data
loader. This custom collate function pads th e training examples in each batch to the
same length while allowing different batches to have di fferent lengths, as demon-
strated in figure 7.8. This approach mini mizes unnecessary paddin g by only extending
sequences to match the longest one in  each batch, not the whole dataset.
Figure 7.8 The padding of training examples in batches using token ID 50256  to ensure uniform length within 
each batch. Each batch may have different lengths, as shown by the first and second.[ 0,     1,     2,     3,     4]
[ 5,     6]
[7,      8,     9][    0,     1,     2,     3,     4]
[    5,     6, 50256, 50256, 50256]
[    7,     8,     9, 50256, 50256]Input 1
Input 2
Input 3
[ 8,     1]
[10,     3,     11,     6]
[ 5,     22,    13,    13]Input 4
Input 5
Input 6[    8,     1, 50256, 50256]
[   10,     3,    11,     6]
[    5,    22,    13,    13]The ﬁrst
batch
The
secondbatchToken ID 50256 is used
as the padding token.Token IDs corresponding to
the ﬁrst training examplePad all training examples in a batch
so that they have the same length.
Licensed to Jason Walsh <manning@jwalsh.net>

=== PAGE 20 ===
219 7.3 Organizing data into training batches
Figure 7.11 The five substeps involved in implementing the batching process. After creating the 
target sequence by shifting token IDs one position to  the right and appending an end-of-text token, in 
step 2.5, we replace the end-of-text padd ing tokens with a placeholder value ( -100 ).2.1) Format data
using prompt
template.
2.2) Tokenize
formatted data.
2.5) Replace
padding tokens
with placeholders.2.3) hAdjust to t e
same length with
padding tokens.Below is an instruction that describes a task. Write a
response that appropriately completes the request.
[21106, 318, 281, 12064, 326,
8477, 257, 4876, 13, ...,]
2.4) Create target
token IDs for
training.[21106, 318, 281, 12064, 326,
8477, 257, 4876, 13, ...,50256, 50256, 50256 ]
[21106, 318, 281, 12064, 326,
8477, 257, 4876, 13, ...,50256, 50256, 50256 ]
[318, 281, 12064, 326, 8477,257, 4876, 13, ...,50256, 50256, 50256, 50256 ]
[21106, 318, 281, 12064, 326,
8477, 257, 4876, 13, ...,50256, 50256, 50256 ]
[318, 281, 12064, 326, 8477,257, 4876, 13, ...,50256, -100, -100, -100 ]Format input into an instruction-
response template.
Convert instruction-response
entry into token IDs.
Add end-of-text tokens (50256)
to pad data samples to the samelength.
Create a list of target token IDs
for the model to learn (these arethe inputs shifted by 1, plus anadditional padding token).
Replace certain padding tokens
by -100 to exclude them fromthe training loss.### Instruction: …
### Response: …
Figure 7.12 Step 2.4 in the token replacement process in t he target batch for the training data preparation. We 
replace all but the first instance of the end-of-text tok en, which we use as padding, with the placeholder value 
-100 , while keeping the initial end-of-text token in each target sequence.[    1,     2,     3,     4, 50256    ] Target 1
[    6, 50256, 50256, 50256, 50256    ] Target 2
[    8,     9, 50256, 50256, 50256    ] Target 3[    1,     2,     3,     4, 50256     ]
[    6, 50256,  -100,  -100,   -100    ]
[    8,     9, 50256,  -100,   -100    ]
We don’t modify the ﬁrst
instance of the end-of-text(padding) token.We replace all but the ﬁrst
instance of the end-of-text(padding) token with -100.
Licensed to Jason Walsh <manning@jwalsh.net>
