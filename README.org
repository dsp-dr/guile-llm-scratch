#+TITLE: Guile LLM from Scratch - Pure Scheme Transformer Implementation
#+AUTHOR: DSP-DR Team
#+DATE: 2025-08-20

#+BEGIN_EXPORT html
<p align="center">
  <img src="https://img.shields.io/badge/guile-3.0.10+-blue.svg" alt="Guile 3.0.10+">
  <img src="https://img.shields.io/badge/ML-transformer-purple.svg" alt="Transformer">
  <img src="https://img.shields.io/badge/dependencies-zero-brightgreen.svg" alt="Zero Dependencies">
  <img src="https://img.shields.io/badge/license-MIT-green.svg" alt="MIT License">
</p>
#+END_EXPORT

* Overview

Pure Guile3 implementation of transformer-based language models from scratch. This project implements the complete LLM architecture including tensor operations, attention mechanisms, and training loops without any external machine learning dependencies - everything built using Scheme's functional programming paradigm.

** Key Features
- ðŸ§® *Tensor Operations*: SRFI-4 optimized numerical arrays with broadcasting
- ðŸ”¤ *BPE Tokenizer*: Byte-pair encoding implementation from scratch
- ðŸŽ¯ *Attention Mechanisms*: Multi-head scaled dot-product attention
- ðŸ¤– *Transformer Architecture*: Complete encoder/decoder implementation
- ðŸ“š *Literate Programming*: Org-mode with Babel for documentation-first development
- ðŸ§ª *Comprehensive Testing*: SRFI-64 test suites for all modules
- ðŸš€ *Zero Dependencies*: Pure Scheme implementation, no external ML libraries

* Architecture

#+BEGIN_SRC mermaid :exports results :file architecture.png
graph TB
    subgraph "Foundation Layer"
        A[Tensor Operations<br/>SRFI-4 Arrays] --> B[Matrix Math<br/>Broadcasting]
    end
    
    subgraph "NLP Layer"
        C[BPE Tokenizer] --> D[Vocabulary<br/>Management]
    end
    
    subgraph "Neural Network Layer"
        E[Attention<br/>Mechanisms] --> F[Multi-Head<br/>Attention]
        G[Feed-Forward<br/>Networks] --> H[Activations<br/>GELU/ReLU]
    end
    
    subgraph "Model Layer"
        I[Transformer<br/>Encoder] --> K[Text Generation]
        J[Transformer<br/>Decoder] --> K
    end
    
    B --> E
    D --> I
    F --> I
    H --> J
#+END_SRC

* Installation

** Prerequisites
- FreeBSD 14.3 or later
- Guile 3.0.10 or later
- GNU Make 4.4+

** Building from Source
#+BEGIN_SRC bash
# Clone the repository
git clone https://github.com/dsp-dr/guile-llm-scratch.git
cd guile-llm-scratch

# Tangle literate org files to generate Scheme code
gmake tangle

# Build the project
gmake all

# Run tests
gmake test

# Start REPL for interactive development
gmake repl
#+END_SRC

* Usage

** Basic Example
#+BEGIN_SRC scheme
(use-modules (core tensor)
             (core tokenizer)
             (core transformer))

;; Create a simple tensor
(define t (tensor-from-list '((1 2 3) (4 5 6))))
(tensor-shape t) ; => (2 3)

;; Initialize tokenizer and encode text
(define tok (make-tokenizer))
(define tokens (encode tok "Hello, Guile!"))

;; Create a nano transformer model
(define model (make-transformer gpt-nano-config))

;; Generate text (after training)
(generate-text model "Once upon a time" 50)
#+END_SRC

** REPL Development
#+BEGIN_SRC bash
# Start Guile REPL with project modules
guile3 -L src/

# Or use the provided script
./repl.scm
#+END_SRC

* Roadmap to v1.0.0

** v0.1.0 - Core Foundation âœ…
- [X] Tensor operations with SRFI-4 arrays
- [X] BPE tokenizer implementation
- [X] Attention mechanisms
- [X] Transformer architecture
- [X] SRFI-64 test framework

** v0.2.0 - Training Infrastructure (Current)
- [ ] Automatic differentiation
- [ ] Backpropagation implementation
- [ ] Optimizer algorithms (SGD, Adam)

** v0.3.0 - System Integration
- [ ] Module interconnections
- [ ] State management patterns
- [ ] Configuration system

** v0.4.0 - Testing & Quality
- [ ] Full unit test coverage
- [ ] Integration tests
- [ ] Property-based testing

** v0.5.0 - Performance
- [ ] Profiling and benchmarks
- [ ] Critical path optimization
- [ ] Memoization strategies

** v0.6.0 - FreeBSD Features
- [ ] Native system call optimization
- [ ] kqueue integration (if applicable)
- [ ] Jail support (if relevant)

** v0.7.0 - Documentation
- [ ] Complete API documentation
- [ ] Usage guide
- [ ] Migration guide from original

** v0.8.0 - Examples & Demos
- [ ] Real-world usage examples
- [ ] Performance comparisons
- [ ] Integration patterns

** v0.9.0 - Beta Release
- [ ] Community feedback incorporation
- [ ] Edge case handling
- [ ] API stabilization

** v1.0.0 - Production Ready
- [ ] Feature complete
- [ ] Fully documented
- [ ] Performance validated

* Development

** Project Structure
#+BEGIN_EXAMPLE
project-root/
â”œâ”€â”€ README.org             # This file
â”œâ”€â”€ Makefile              # Build automation
â”œâ”€â”€ guix.scm             # Guix package definition
â”œâ”€â”€ .envrc               # direnv configuration
â”œâ”€â”€ src/                 # Source code
â”‚   â”œâ”€â”€ core/           # Core functionality
â”‚   â”œâ”€â”€ utils/          # Utilities
â”‚   â””â”€â”€ main.scm        # Entry point
â”œâ”€â”€ tests/              # Test suite
â”‚   â”œâ”€â”€ core/
â”‚   â””â”€â”€ test-runner.scm
â”œâ”€â”€ docs/               # Literate documentation
â”‚   â”œâ”€â”€ implementation.org
â”‚   â”œâ”€â”€ design.org
â”‚   â”œâ”€â”€ notes.org
â”‚   â””â”€â”€ api.org
â”œâ”€â”€ examples/           # Usage examples
â””â”€â”€ tmp/               # Book PDFs (not in repo)
#+END_EXAMPLE

** Contributing
Contributions are welcome! Please read the contributing guidelines and ensure all tests pass before submitting PRs.

** Building Documentation
#+BEGIN_SRC bash
# Generate HTML documentation
gmake docs-html

# Generate PDF documentation
gmake docs-pdf

# Generate Info documentation
gmake docs-info
#+END_SRC

* License

This project is licensed under the MIT License. See LICENSE file for details.

* Current Implementation Status

** Completed Modules
- âœ… `tensor.scm` - Efficient tensor operations with broadcasting
- âœ… `tokenizer.scm` - BPE tokenization from scratch  
- âœ… `attention.scm` - Scaled dot-product and multi-head attention
- âœ… `transformer.scm` - Complete encoder/decoder architecture

** Next Steps
1. Implement automatic differentiation for backpropagation
2. Add training loop with loss functions (cross-entropy, MSE)
3. Create data loading pipeline for text datasets
4. Build example models (GPT-nano for testing)
5. Performance profiling and optimization

* Contributing

This project uses literate programming - all code is documented in `docs/*.org` files and tangled to generate the Scheme source. To contribute:

1. Edit the relevant `.org` file in `docs/`
2. Run `gmake tangle` to generate Scheme code
3. Run `gmake test` to ensure tests pass
4. Submit PR with both org and generated files

* Acknowledgments

- Inspired by "Building LLMs from Scratch" approaches
- Guile community for excellent documentation and support
- FreeBSD project for a robust operating system
- Collaborators: @jwalsh, @seanjensengrey, @aygp-dr

* License

MIT License - See LICENSE file for details.

* Topics

~guile~ ~scheme~ ~machine-learning~ ~transformer~ ~language-model~ ~literate-programming~ ~freebsd~ ~llm~